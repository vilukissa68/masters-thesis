{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faee1b07-857a-476b-bc08-b1ecc6dfe185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "0\n",
      "-128\n"
     ]
    }
   ],
   "source": [
    "no_acc = [44, 2, 4, 3, 3, 23, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 43, 0, 0, 0, 0, 19, 13, 4, 0, 0, 0, 10, 6, 7, 46, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 0, 1, 0, 0, 5, 0, 0, 0, 0, 0, 19, 0, 9, 4, 0, 25, 4, 5, 0, 0, 0, 35, 0, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 4, 1, 17, 0, 6, 0, 0, 15, 0, 11, 0, 0, 12, 0, 1, 0, 0, 7, 0, 0, 7, 0, 0, 0, 0, 12, 0, 2, 0, 0, 2, 8, 5, 0, 3, 0, 6, 10, 0, 0, 0, 0, 0, -1, -1, -128, -128, -128, -128, -128, -128, -128, 127, -128, -128, -128, -128]\n",
    "for x in range(0,3):\n",
    "    print(no_acc[64*x])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e0ad9965-54e0-4187-8670-59441e6a7c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________\n",
      "tensor([[[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2., 3.],\n",
      "          [4., 5., 6.],\n",
      "          [7., 8., 9.]]]])\n",
      "____________________________________\n",
      "tensor([[[[9., 8., 7.],\n",
      "          [4., 5., 6.],\n",
      "          [1., 2., 3.]]],\n",
      "\n",
      "\n",
      "        [[[9., 8., 7.],\n",
      "          [4., 5., 6.],\n",
      "          [1., 2., 3.]]]])\n",
      "____________________________________\n",
      "tensor([[[[1., 4., 9.],\n",
      "          [2., 5., 8.],\n",
      "          [3., 6., 7.]]],\n",
      "\n",
      "\n",
      "        [[[1., 4., 9.],\n",
      "          [2., 5., 8.],\n",
      "          [3., 6., 7.]]]])\n",
      "____________________________________\n",
      "tensor([[[[9., 4., 1.],\n",
      "          [8., 5., 2.],\n",
      "          [7., 6., 3.]]],\n",
      "\n",
      "\n",
      "        [[[9., 4., 1.],\n",
      "          [8., 5., 2.],\n",
      "          [7., 6., 3.]]]])\n",
      "Output with manually implemented groups: torch.Size([1, 8, 3, 3])\n",
      "Output with manually implemented groups:\n",
      " tensor([[[[ 80., 125., 170.],\n",
      "          [ 80., 125., 170.],\n",
      "          [ 80., 125., 170.]],\n",
      "\n",
      "         [[112., 157., 202.],\n",
      "          [112., 157., 202.],\n",
      "          [112., 157., 202.]],\n",
      "\n",
      "         [[178., 133.,  88.],\n",
      "          [178., 133.,  88.],\n",
      "          [178., 133.,  88.]],\n",
      "\n",
      "         [[178., 133.,  88.],\n",
      "          [178., 133.,  88.],\n",
      "          [178., 133.,  88.]],\n",
      "\n",
      "         [[ 80., 131., 194.],\n",
      "          [ 80., 131., 194.],\n",
      "          [ 80., 131., 194.]],\n",
      "\n",
      "         [[ 64., 115., 178.],\n",
      "          [ 64., 115., 178.],\n",
      "          [ 64., 115., 178.]],\n",
      "\n",
      "         [[102., 141., 117.],\n",
      "          [102., 141., 117.],\n",
      "          [102., 141., 117.]],\n",
      "\n",
      "         [[174., 213., 189.],\n",
      "          [174., 213., 189.],\n",
      "          [174., 213., 189.]]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input tensor (from `din` in Rust code) with shape [N, C_in, H, W]\n",
    "input_tensor = torch.tensor([\n",
    "    [\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [1, 2, 3, 4, 5]\n",
    "    ],\n",
    "    [\n",
    "        [5, 4, 3, 2, 1],\n",
    "        [5, 4, 3, 2, 1],\n",
    "        [5, 4, 3, 2, 1],\n",
    "        [5, 4, 3, 2, 1],\n",
    "        [5, 4, 3, 2, 1]\n",
    "    ],\n",
    "    [\n",
    "        [3, 2, 1, 4, 5],\n",
    "        [3, 2, 1, 4, 5],\n",
    "        [3, 2, 1, 4, 5],\n",
    "        [3, 2, 1, 4, 5],\n",
    "        [3, 2, 1, 4, 5]\n",
    "    ], \n",
    "    [\n",
    "        [2, 4, 5, 1, 3],\n",
    "        [2, 4, 5, 1, 3],\n",
    "        [2, 4, 5, 1, 3],\n",
    "        [2, 4, 5, 1, 3],\n",
    "        [2, 4, 5, 1, 3],\n",
    "    ]\n",
    "], dtype=torch.int8).unsqueeze(0).float()  # Shape: [1, 3, 5, 5]\n",
    "\n",
    "# Define the Conv2d layer with groups (4 groups for 4 channels in the example)\n",
    "conv_group_2 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, groups=1)\n",
    "conv_group_manual = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, groups=1)\n",
    "\n",
    "# Convert Rust `wgt` to PyTorch tensor and reshape for the conv layers\n",
    "wgt = torch.tensor([\n",
    "    [[1,2,3], [4,5,6], [7,8,9]],\n",
    "    [[9,8,7], [4,5,6], [1,2,3]],\n",
    "    [[1,4,9], [2,5,8], [3,6,7]],\n",
    "    [[9,4,1], [8,5,2], [7,6,3]],\n",
    "    \n",
    "    [[1,2,3], [4,5,6], [7,8,9]],\n",
    "    [[9,8,7], [4,5,6], [1,2,3]],\n",
    "    [[1,4,9], [2,5,8], [3,6,7]],\n",
    "    [[9,4,1], [8,5,2], [7,6,3]]\n",
    "], dtype=torch.int8).reshape(2, 4, 3, 3).float()  # Shape: [8, 3, 3, 3]\n",
    "\n",
    "# Convert Rust `bias` to PyTorch tensor\n",
    "bias = torch.tensor([-16, 16, 0, 0, 8, -8, -36, 36], dtype=torch.int16).float()\n",
    "\n",
    "# Apply weights and bias to the conv_group_2 layer\n",
    "conv_group_2.weight = nn.Parameter(wgt)\n",
    "conv_group_2.bias = nn.Parameter(bias)\n",
    "\n",
    "# Manually implemented version with separate conv layers and concatenation\n",
    "class ManualGroupedConv(nn.Module):\n",
    "    def __init__(self, weights, biases):\n",
    "        super(ManualGroupedConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.conv1.weight = nn.Parameter(weights[0])\n",
    "        self.conv2.weight = nn.Parameter(weights[1])\n",
    "        self.conv3.weight = nn.Parameter(weights[2])\n",
    "        self.conv4.weight = nn.Parameter(weights[3])\n",
    "        self.conv1.bias = nn.Parameter(biases[0])\n",
    "        self.conv2.bias = nn.Parameter(biases[1])\n",
    "        self.conv3.bias = nn.Parameter(biases[2])\n",
    "        self.conv4.bias = nn.Parameter(biases[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        splits = torch.split(x, 1, dim=1)  # Split channels\n",
    "        out1 = self.conv1(splits[0])\n",
    "        out2 = self.conv2(splits[1])\n",
    "        out3 = self.conv3(splits[2])\n",
    "        out4 = self.conv4(splits[3])\n",
    "        return torch.cat([out1, out2, out3, out4], dim=1)  # Concatenate the outputs\n",
    "\n",
    "# Create manual group conv with weights and biases\n",
    "weights = wgt.chunk(4, dim=1)\n",
    "for w in weights:\n",
    "    print(\"____________________________________\")\n",
    "    print(w)\n",
    "biases = bias.chunk(4)\n",
    "manual_group_conv = ManualGroupedConv(weights, biases)\n",
    "\n",
    "# Apply the layers to the input tensor\n",
    "output_manual = manual_group_conv(input_tensor)\n",
    "\n",
    "# Compare the outputs\n",
    "print(f\"Output with manually implemented groups: {output_manual.shape}\")\n",
    "\n",
    "# Print outputs\n",
    "print(\"Output with manually implemented groups:\\n\", output_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94e253fa-51db-4598-9f29-c2472ab535d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[-0.1996, -0.0541, -0.0578],\n",
      "          [ 0.3317, -0.0094,  0.1468],\n",
      "          [-0.0404, -0.1715, -0.2873]]],\n",
      "\n",
      "\n",
      "        [[[-0.1923, -0.2847,  0.2676],\n",
      "          [ 0.0068, -0.3035, -0.0364],\n",
      "          [-0.2271,  0.3301, -0.1119]]]]), tensor([[[[-0.1875,  0.0898,  0.3057],\n",
      "          [ 0.1733,  0.2023, -0.2250],\n",
      "          [-0.1055,  0.2657, -0.2829]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2047,  0.1589,  0.2478],\n",
      "          [ 0.2174,  0.2594, -0.2384],\n",
      "          [ 0.1749,  0.0625,  0.0872]]]]), tensor([[[[ 0.0547,  0.2717,  0.2748],\n",
      "          [ 0.3173,  0.2304,  0.3139],\n",
      "          [-0.0540, -0.1187, -0.0311]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0020,  0.1235,  0.0810],\n",
      "          [-0.0303,  0.3200,  0.1364],\n",
      "          [ 0.3146,  0.0158,  0.3014]]]]), tensor([[[[ 0.2170, -0.2991,  0.0782],\n",
      "          [ 0.3103,  0.0825, -0.0578],\n",
      "          [ 0.0687,  0.2704,  0.2271]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3098, -0.0075, -0.2496],\n",
      "          [-0.3150,  0.2385, -0.1995],\n",
      "          [-0.2754, -0.2036,  0.0391]]]]))\n",
      "(tensor([-0.2930,  0.1524]), tensor([-0.2642,  0.3159]), tensor([-0.1158, -0.2564]), tensor([0.1091, 0.2757]))\n",
      "Output with Conv2d groups: torch.Size([1, 8, 3, 3])\n",
      "Output with manually implemented groups: torch.Size([1, 8, 3, 3])\n",
      "Outputs are equal: False\n",
      "tensor([[[-0.8446, -0.6891, -0.7678],\n",
      "         [ 0.6683,  0.1197,  0.4171],\n",
      "         [-0.3517, -0.2460, -0.7836]],\n",
      "\n",
      "        [[-0.1679,  0.2686, -1.0465],\n",
      "         [-0.2170,  1.1068, -0.5988],\n",
      "         [ 0.0521,  0.4377, -0.0935]],\n",
      "\n",
      "        [[-0.5778, -0.9149, -0.8233],\n",
      "         [-0.0256,  0.6037,  0.1390],\n",
      "         [ 0.3065, -0.9277, -0.4014]],\n",
      "\n",
      "        [[-0.7042,  0.0361, -0.0336],\n",
      "         [-0.8143,  0.0921, -0.3850],\n",
      "         [-0.4388, -0.0128,  0.4417]],\n",
      "\n",
      "        [[ 0.6863, -0.6039, -0.1723],\n",
      "         [-0.0027,  0.2917,  0.1365],\n",
      "         [ 0.7207,  0.8061,  0.2228]],\n",
      "\n",
      "        [[ 0.4411, -0.2318,  0.8435],\n",
      "         [-0.1144, -0.8644,  0.2698],\n",
      "         [-0.3006, -0.5853, -0.0479]],\n",
      "\n",
      "        [[ 0.3368,  0.2035, -0.7643],\n",
      "         [ 0.1905,  1.0150,  0.3977],\n",
      "         [ 0.6332, -0.7835, -0.4148]],\n",
      "\n",
      "        [[ 0.7582,  0.2972, -0.1288],\n",
      "         [-0.6788,  0.4955, -1.3071],\n",
      "         [ 0.2512,  0.3597,  1.3398]]], grad_fn=<UnbindBackward0>)\n",
      "tensor([[[[-0.8446, -0.6891, -0.7678],\n",
      "          [ 0.6683,  0.1197,  0.4171],\n",
      "          [-0.3517, -0.2460, -0.7836]],\n",
      "\n",
      "         [[-0.1679,  0.2686, -1.0465],\n",
      "          [-0.2170,  1.1068, -0.5988],\n",
      "          [ 0.0521,  0.4377, -0.0935]],\n",
      "\n",
      "         [[-0.5778, -0.9149, -0.8233],\n",
      "          [-0.0256,  0.6037,  0.1390],\n",
      "          [ 0.3065, -0.9277, -0.4014]],\n",
      "\n",
      "         [[-0.7042,  0.0361, -0.0336],\n",
      "          [-0.8143,  0.0921, -0.3850],\n",
      "          [-0.4388, -0.0128,  0.4417]],\n",
      "\n",
      "         [[ 0.6863, -0.6039, -0.1723],\n",
      "          [-0.0027,  0.2917,  0.1365],\n",
      "          [ 0.7207,  0.8061,  0.2228]],\n",
      "\n",
      "         [[ 0.4411, -0.2318,  0.8435],\n",
      "          [-0.1144, -0.8644,  0.2698],\n",
      "          [-0.3006, -0.5853, -0.0479]],\n",
      "\n",
      "         [[ 0.3368,  0.2035, -0.7643],\n",
      "          [ 0.1905,  1.0150,  0.3977],\n",
      "          [ 0.6332, -0.7835, -0.4148]],\n",
      "\n",
      "         [[ 0.7582,  0.2972, -0.1288],\n",
      "          [-0.6788,  0.4955, -1.3071],\n",
      "          [ 0.2512,  0.3597,  1.3398]]]], grad_fn=<ConvolutionBackward0>)\n",
      "tensor([[[[-0.4754, -0.2090, -0.3308],\n",
      "          [-0.0144, -1.0024,  0.0419],\n",
      "          [-0.4052, -0.6400,  0.2110]],\n",
      "\n",
      "         [[-0.4563,  1.3319, -0.1805],\n",
      "          [ 0.1580,  0.5190, -0.1409],\n",
      "          [ 0.4238, -0.2590,  0.2612]],\n",
      "\n",
      "         [[ 0.6882, -1.1673, -0.0953],\n",
      "          [-0.8870,  0.4678, -1.2543],\n",
      "          [-0.0720, -0.1143, -1.1448]],\n",
      "\n",
      "         [[-0.3715, -0.2661,  0.0422],\n",
      "          [-0.0616,  0.2675,  0.8379],\n",
      "          [-1.5011, -0.0074, -1.2526]],\n",
      "\n",
      "         [[-0.2723,  0.1865,  0.7580],\n",
      "          [ 0.0605, -0.4994, -0.8632],\n",
      "          [ 0.5621,  0.3377,  0.4351]],\n",
      "\n",
      "         [[-0.1315, -0.4747, -0.4785],\n",
      "          [ 0.5010, -0.0409, -0.0126],\n",
      "          [-0.1400, -0.4838,  0.0487]],\n",
      "\n",
      "         [[ 0.0124,  0.2989, -0.0086],\n",
      "          [ 0.4973,  0.0916,  0.2972],\n",
      "          [ 0.4289, -0.8218,  0.2515]],\n",
      "\n",
      "         [[-0.1305,  0.1982,  0.8197],\n",
      "          [-0.4367,  0.2540,  0.3265],\n",
      "          [-0.5686,  0.6657,  0.9005]]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.randn(1, 4, 5, 5)  # Example input with 4 channels\n",
    "\n",
    "# Define the Conv2d layer with groups\n",
    "conv_group = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, groups=4)\n",
    "conv_group_2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, groups=1)\n",
    "conv_group_manual = copy.deepcopy(conv_group)  # Ensure they start with the same weights and biases\n",
    "\n",
    "# Manually implemented version with separate conv layers and concatenation\n",
    "class ManualGroupedConv(nn.Module):\n",
    "    def __init__(self, weights, biases):\n",
    "        super(ManualGroupedConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.conv1.weight = nn.Parameter(weights[0])\n",
    "        self.conv2.weight = nn.Parameter(weights[1])\n",
    "        self.conv3.weight = nn.Parameter(weights[2])\n",
    "        self.conv4.weight = nn.Parameter(weights[3])\n",
    "        self.conv1.bias = nn.Parameter(biases[0])\n",
    "        self.conv2.bias = nn.Parameter(biases[1])\n",
    "        self.conv3.bias = nn.Parameter(biases[2])\n",
    "        self.conv4.bias = nn.Parameter(biases[3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        splits = torch.split(x, 1, dim=1)  # Split channels\n",
    "        out1 = self.conv1(splits[0])\n",
    "        out2 = self.conv2(splits[1])\n",
    "        out3 = self.conv3(splits[2])\n",
    "        out4 = self.conv4(splits[3])\n",
    "        return torch.cat([out1, out2, out3, out4], dim=1)  # Concatenate the outputs\n",
    "\n",
    "# Extract weights and biases from grouped Conv2d layer\n",
    "weights = conv_group.weight.data.chunk(4, dim=0)\n",
    "biases = conv_group.bias.data.chunk(4, dim=0)\n",
    "\n",
    "# Create manually implemented group conv layer with initialized weights and biases\n",
    "manual_group_conv = ManualGroupedConv(weights, biases)\n",
    "\n",
    "# Apply the layers to the input tensor\n",
    "#output_group = conv_group_manual(input_tensor)\n",
    "output_group = conv_group_2(input_tensor)\n",
    "output_manual = manual_group_conv(input_tensor)\n",
    "\n",
    "\n",
    "print(weights)\n",
    "print(biases)\n",
    "\n",
    "# Compare the outputs\n",
    "print(f\"Output with Conv2d groups: {output_group.shape}\")\n",
    "print(f\"Output with manually implemented groups: {output_manual.shape}\")\n",
    "print(f\"Outputs are equal: {torch.allclose(output_group, output_manual)}\")\n",
    "for x in output_group:\n",
    "    print(x)\n",
    "print(output_group)\n",
    "print(output_manual)\n",
    "# In_channels = in_channels / groups\n",
    "# Out_channels = out_channels / groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd068597-b032-44d2-8221-420657cda99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb476c8-7169-4376-9b61-1071cf6fc409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
