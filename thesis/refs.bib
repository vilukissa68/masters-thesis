
@manual{newlib,
  title     = {Newlib: A C Library},
  author    = {Red Hat, Inc. and Various Contributors},
  year      = {2023},
  note      = {\url{https://sourceware.org/newlib/}},
  howpublished = {Online: \url{https://sourceware.org/newlib/}},
  abstract  = {Newlib is a C library intended for use on embedded systems, providing ANSI C standard library functions.}
}

@Book{DeepLearningBook,
  Title                    = {Deep Learning},
  Author                   = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},
  Address                  = {Cambridge, MA, USA},
  Note                     = {\url{http://www.deeplearningbook.org}}
}

@inproceedings{TVM,
  author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  title = {TVM: an automated end-to-end optimizing compiler for deep learning},
  year = {2018},
  isbn = {9781931971478},
  publisher = {USENIX Association},
  address = {USA},
  abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms - such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) - requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
  booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
  pages = {579–594},
  numpages = {16},
  location = {Carlsbad, CA, USA},
  series = {OSDI'18}
}

@inproceedings{Ballast,
  author = {Rautakoura, A and Hamalainen, T and Kulmala, A and Lehtinen, Tero and Duman, Mehdi and Ibrahim, Mohamed},
  copyright = {This publication is copyrighted. You may download, display and print it for Your own personal use. Commercial use is prohibited},
  keywords = {113 Computer and information sciences},
  language = {eng},
  organization = {Fabelo, H},
  title = {Ballast : Implementation of a Large MP-SoC on 22nm ASIC Technology},
  year = {2022},
}

@article{tinyperf,
  title={MLPerf Tiny Benchmark},
  author={Banbury, Colby and Reddi, Vijay Janapa and Torelli, Peter and Holleman, Jeremy and Jeffries, Nat and Kiraly, Csaba and Montino, Pietro and Kanter, David and Ahmed, Sebastian and Pau, Danilo and others},
  journal={Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
  year={2021}
}
@Article{wolf08,
  author = 	 {W. Wolf and A.A. Jerraya and G. Martin},
  title = 	 {Multiprocessor System-on-Chip{(MPSoC)} Technology},
  journal = 	 t-cadics,
  year = 	 {2008},
  OPTkey = 	 {},
  volume = 	 {27},
  number = 	 {10},
  pages = 	 {1701--1713},
  month = 	 {Oct.},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@techreport{bennett2010porting,
  title        = {Howto: Porting Newlib: A Simple Guide},
  author       = {Jeremy Bennett},
  institution  = {Embecosm},
  number       = {Application Note 9},
  edition      = {Issue 1},
  year         = {2010},
  month        = {July},
  note         = {Licensed under Creative Commons Attribution 2.0 UK: England \& Wales License},
  url          = {https://www.embecosm.com/appnotes/ean9/html/index.html},
}

@manual{linux_times_man_page,
  title        = {{Linux Man Pages: times(2)}},
  organization = {Linux},
  year         = {2024},
  note         = {Available at \url{https://man7.org/linux/man-pages/man2/times.2.html}},
}

@inproceedings{rectifier,
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Y.},
year = {2010},
month = {01},
pages = {},
title = {Deep Sparse Rectifier Neural Networks},
volume = {15},
journal = {Journal of Machine Learning Research}
}

@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition},
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385},
}

@misc{howard2017mobilenetsefficientconvolutionalneural,
      title={MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
      author={Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
      year={2017},
      eprint={1704.04861},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1704.04861},
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1912.01703},
}

@misc{Cifar10Krizhevsky09learningmultiple,
  author = {Alex Krizhevsky},
  title = {Learning multiple layers of features from tiny images},
  institution = {},
  year = {2009},
}
