
@misc{ridoy_compressed_2024,
	title = {Compressed {Image} {Captioning} using {CNN}-based {Encoder}-{Decoder} {Framework}},
	url = {http://arxiv.org/abs/2404.18062},
	doi = {10.48550/arXiv.2404.18062},
	abstract = {In today's world, image processing plays a crucial role across various fields, from scientific research to industrial applications. But one particularly exciting application is image captioning. The potential impact of effective image captioning is vast. It can significantly boost the accuracy of search engines, making it easier to find relevant information. Moreover, it can greatly enhance accessibility for visually impaired individuals, providing them with a more immersive experience of digital content. However, despite its promise, image captioning presents several challenges. One major hurdle is extracting meaningful visual information from images and transforming it into coherent language. This requires bridging the gap between the visual and linguistic domains, a task that demands sophisticated algorithms and models. Our project is focused on addressing these challenges by developing an automatic image captioning architecture that combines the strengths of convolutional neural networks (CNNs) and encoder-decoder models. The CNN model is used to extract the visual features from images, and later, with the help of the encoder-decoder framework, captions are generated. We also did a performance comparison where we delved into the realm of pre-trained CNN models, experimenting with multiple architectures to understand their performance variations. In our quest for optimization, we also explored the integration of frequency regularization techniques to compress the "AlexNet" and "EfficientNetB0" model. We aimed to see if this compressed model could maintain its effectiveness in generating image captions while being more resource-efficient.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Ridoy, Md Alif Rahman and Hasan, M. Mahmud and Bhowmick, Shovon},
	month = apr,
	year = {2024},
	note = {arXiv:2404.18062 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/vainogranat/Zotero/storage/U3SMCPTX/Ridoy et al. - 2024 - Compressed Image Captioning using CNN-based Encode.pdf:application/pdf;arXiv.org Snapshot:/Users/vainogranat/Zotero/storage/DDXC65WV/2404.html:text/html},
}

@misc{banbury_mlperf_2021,
	title = {{MLPerf} {Tiny} {Benchmark}},
	url = {http://arxiv.org/abs/2106.07597},
	doi = {10.48550/arXiv.2106.07597},
	abstract = {Advancements in ultra-low-power tiny machine learning (TinyML) systems promise to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted and easily reproducible benchmark for these systems. To meet this need, we present MLPerf Tiny, the first industry-standard benchmark suite for ultra-low-power tiny machine learning systems. The benchmark suite is the collaborative effort of more than 50 organizations from industry and academia and reflects the needs of the community. MLPerf Tiny measures the accuracy, latency, and energy of machine learning inference to properly evaluate the tradeoffs between systems. Additionally, MLPerf Tiny implements a modular design that enables benchmark submitters to show the benefits of their product, regardless of where it falls on the ML deployment stack, in a fair and reproducible manner. The suite features four benchmarks: keyword spotting, visual wake words, image classification, and anomaly detection.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Banbury, Colby and Reddi, Vijay Janapa and Torelli, Peter and Holleman, Jeremy and Jeffries, Nat and Kiraly, Csaba and Montino, Pietro and Kanter, David and Ahmed, Sebastian and Pau, Danilo and Thakker, Urmish and Torrini, Antonio and Warden, Peter and Cordaro, Jay and Di Guglielmo, Giuseppe and Duarte, Javier and Gibellini, Stephen and Parekh, Videet and Tran, Honson and Tran, Nhan and Wenxu, Niu and Xuesong, Xu},
	month = aug,
	year = {2021},
	note = {arXiv:2106.07597 [cs]},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning},
	annote = {Comment: TinyML Benchmark},
	file = {arXiv Fulltext PDF:/Users/vainogranat/Zotero/storage/KCE5NA4B/Banbury et al. - 2021 - MLPerf Tiny Benchmark.pdf:application/pdf;arXiv.org Snapshot:/Users/vainogranat/Zotero/storage/9VT773FE/2106.html:text/html},
}

@misc{chen_tvm_2018,
	title = {{TVM}: {An} {Automated} {End}-to-{End} {Optimizing} {Compiler} for {Deep} {Learning}},
	shorttitle = {{TVM}},
	url = {http://arxiv.org/abs/1802.04799},
	doi = {10.48550/arXiv.1802.04799},
	abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
	month = oct,
	year = {2018},
	note = {arXiv:1802.04799 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Programming Languages, TVM},
	annote = {Comment: Significantly improved version, add automated optimization},
	file = {arXiv Fulltext PDF:/Users/vainogranat/Zotero/storage/BCVMWEEG/Chen et al. - 2018 - TVM An Automated End-to-End Optimizing Compiler f.pdf:application/pdf;arXiv.org Snapshot:/Users/vainogranat/Zotero/storage/AY9V3SGG/1802.html:text/html},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	url = {http://www.deeplearning.org},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@misc{warden_speech_2018,
	title = {Speech {Commands}: {A} {Dataset} for {Limited}-{Vocabulary} {Speech} {Recognition}},
	shorttitle = {Speech {Commands}},
	url = {http://arxiv.org/abs/1804.03209},
	doi = {10.48550/arXiv.1804.03209},
	abstract = {Describes an audio dataset of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that is different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions and properties. Concludes by reporting baseline results of models trained on this dataset.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Warden, Pete},
	month = apr,
	year = {2018},
	note = {arXiv:1804.03209 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/vainogranat/Zotero/storage/VACCDMUI/Warden - 2018 - Speech Commands A Dataset for Limited-Vocabulary .pdf:application/pdf;arXiv.org Snapshot:/Users/vainogranat/Zotero/storage/9B5PF6ZZ/1804.html:text/html},
}

@misc{liu_deploying_2023,
	title = {Deploying {Machine} {Learning} {Models} to {Ahead}-of-{Time} {Runtime} on {Edge} {Using} {MicroTVM}},
	url = {http://arxiv.org/abs/2304.04842},
	abstract = {In the past few years, more and more AI applications have been applied to edge devices. However, models trained by data scientists with machine learning frameworks, such as PyTorch or TensorFlow, can not be seamlessly executed on edge. In this paper, we develop an end-to-end code generator parsing a pre-trained model to C source libraries for the backend using MicroTVM, a machine learning compiler framework extension addressing inference on bare metal devices. An analysis shows that speciﬁc compute-intensive operators can be easily ofﬂoaded to the dedicated accelerator with a Universal Modular Accelerator (UMA) interface, while others are processed in the CPU cores. By using the automatically generated ahead-of-time C runtime, we conduct a hand gesture recognition experiment on an ARM Cortex M4F core.},
	language = {en},
	urldate = {2024-09-04},
	publisher = {arXiv},
	author = {Liu, Chen and Jobst, Matthias and Guo, Liyuan and Shi, Xinyue and Partzsch, Johannes and Mayr, Christian},
	month = apr,
	year = {2023},
	note = {arXiv:2304.04842 [cs]},
	keywords = {Computer Science - Machine Learning, TVM},
	annote = {Comment: CODAI 2022 Workshop - Embedded System Week (ESWeek)},
	file = {Liu et al. - 2023 - Deploying Machine Learning Models to Ahead-of-Time.pdf:/Users/vainogranat/Zotero/storage/PKNVKUEE/Liu et al. - 2023 - Deploying Machine Learning Models to Ahead-of-Time.pdf:application/pdf},
}

@inproceedings{ahmadifarsani_towards_2023,
	address = {Hamburg Germany},
	title = {Towards {Rapid} {Exploration} of {Heterogeneous} {TinyML} {Systems} using {Virtual} {Platforms} and {TVM}'s {UMA}},
	isbn = {9798400703379},
	url = {https://dl.acm.org/doi/10.1145/3615338.3618121},
	doi = {10.1145/3615338.3618121},
	abstract = {The rapid setup of deep learning compilation toolchains for heterogeneous TinyML systems with a processor and dedicated ML accelerator is still at an early stage. Here, achieving the most optimal combination of targets for a TinyML application on ultra-low-power edge devices demands additional benchmarking solutions to estimate the final performance. Apache TVM’s Universal Modular Accelerator (UMA) interface as an easy-to-use API is a promising speed-up approach to this scope. In this paper, we integrate a simple custom dedicated accelerator into TVM using UMA to offload the quantized convolution operators in order to demonstrate such an approach. Furthermore, we leverage MLonMCU tool and its capability of virtual prototyping to estimate and explore the performance improvement achieved by the accelerator.},
	language = {en},
	urldate = {2024-09-05},
	booktitle = {Proceedings of the 2023 {Workshop} on {Compilers}, {Deployment}, and {Tooling} for {Edge} {AI}},
	publisher = {ACM},
	author = {Ahmadifarsani, Samira and Stahl, Rafael and Van Kempen, Philipp and Mueller-Gritschneder, Daniel and Schlichtmann, Ulf},
	month = sep,
	year = {2023},
	keywords = {TVM},
	pages = {6--10},
	file = {Ahmadifarsani et al. - 2023 - Towards Rapid Exploration of Heterogeneous TinyML .pdf:/Users/vainogranat/Zotero/storage/7W6YS46C/Ahmadifarsani et al. - 2023 - Towards Rapid Exploration of Heterogeneous TinyML .pdf:application/pdf},
}

@misc{jain_efficient_2020,
	title = {Efficient {Execution} of {Quantized} {Deep} {Learning} {Models}: {A} {Compiler} {Approach}},
	shorttitle = {Efficient {Execution} of {Quantized} {Deep} {Learning} {Models}},
	url = {http://arxiv.org/abs/2006.10226},
	abstract = {A growing number of applications implement predictive functions using deep learning models, which require heavy use of compute and memory. For deep learning workloads to run well on a broad range of systems from cloud-scale clusters to low-power edge devices, they need to use available compute and memory resources more efﬁciently. One popular technique for increasing resource efﬁciency is 8-bit integer quantization, in which 32-bit ﬂoating point numbers (fp32) are represented using shorter 8-bit integer numbers. Although deep learning frameworks such as TensorFlow, TFLite, MXNet, and PyTorch enable developers to quantize models with only a small drop in accuracy, they are not well suited to execute quantized models on a variety of hardware platforms. For example, TFLite is optimized to run inference on ARM CPU edge devices but it does not have efﬁcient support for Intel CPUs and Nvidia GPUs. In this paper, we address the challenges of executing quantized deep learning models on diverse hardware platforms by proposing an augmented compiler approach. A deep learning compiler such as Apache TVM can enable the efﬁcient execution of model from various frameworks on various targets. Many deep learning compilers today, however, are designed primarily for fp32 computation and cannot optimize a pre-quantized INT8 model. To address this issue, we created a new dialect called Quantized Neural Network (QNN) that extends the compiler’s internal representation with a quantization context. With this quantization context, the compiler can generate efﬁcient code for pre-quantized models on various hardware platforms. As implemented in Apache TVM, we observe that the QNNaugmented deep learning compiler achieves speedups of 2.35×, 2.15×, 1.35× and 1.40× on Intel Xeon Cascade Lake CPUs, Nvidia Tesla T4 GPUs, ARM Cortex-A CPUs on Raspberry Pi3 and Pi4 respectively against well optimized fp32 execution. The use of QNN with compilation of pre-quantized models enables developers to achieve model execution performance comparable to the state-of-the-art framework-speciﬁc solutions but on a wider range of hardware platforms.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Jain, Animesh and Bhattacharya, Shoubhik and Masuda, Masahiro and Sharma, Vin and Wang, Yida},
	month = jun,
	year = {2020},
	note = {arXiv:2006.10226 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Distributed, Parallel, and Cluster Computing, TVM},
	file = {Jain et al. - 2020 - Efficient Execution of Quantized Deep Learning Mod.pdf:/Users/vainogranat/Zotero/storage/YBYQE9SI/Jain et al. - 2020 - Efficient Execution of Quantized Deep Learning Mod.pdf:application/pdf},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://arxiv.org/abs/1512.03385v1},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	language = {en},
	urldate = {2024-10-03},
	journal = {arXiv.org},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/LXYD49HB/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{howard_mobilenets_2017,
	title = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
	shorttitle = {{MobileNets}},
	url = {https://arxiv.org/abs/1704.04861v1},
	abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
	language = {en},
	urldate = {2024-10-03},
	journal = {arXiv.org},
	author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
	month = apr,
	year = {2017},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/M5H6NTC6/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf:application/pdf},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2024-10-07},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
	annote = {Comment: 12 pages, 3 figures, NeurIPS 2019},
	file = {arXiv Fulltext PDF:/Users/vainogranat/Zotero/storage/BLZZS8AI/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/vainogranat/Zotero/storage/366GXI7P/1912.html:text/html},
}

@article{shaheed_ds-cnn_2022,
	title = {{DS}-{CNN}: {A} pre-trained {Xception} model based on depth-wise separable convolutional neural network for finger vein recognition},
	volume = {191},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417421015943},
	doi = {https://doi.org/10.1016/j.eswa.2021.116288},
	abstract = {Finger vein recognition received special attention among all other biometric traits due to its high security. Adequate recognition and classification accuracy ensure the security of personal authentication. Many convolutional neural networks (CNNs) have been proposed with a promising performance in biometric finger vein recognition. However, their architectures have several problems, such as high complexity, extraction of robust features, degraded performance, etc. Considering the issues of CNNs, the authors present a pre-trained CNN network named Xception model based on depth-wise separable CNNs with residual connection, which is considered to be a more effective, less complex neural network to extract robust features. Our work can be seen as a three-stage process. Initially, the concept of data pre-processing is applied to convert the raw input samples into the standard format. Afterward, data augmentation using different geometrical techniques is incorporated to overcome the lack of training samples required for training the deep learning model. Finally, the feature extraction and classification task is performed through the pre-trained Xception architecture to verify the person's identity. SDUMLA and THU-FVFDT2 datasets are utilized to test and evaluate the proposed multi-layered CNN model performance with existing arts. Our proposed method for the SDUMLA database achieved an accuracy of 99\% with an F1-score of 98\%. While on THU-FVFDT2, the proposed method obtained an accuracy of 90\% with an F1-score of 88\%. Experimental results conclude that the proposed work obtained excellent performance compared to existing methods.},
	journal = {Expert Systems with Applications},
	author = {Shaheed, Kashif and Mao, Aihua and Qureshi, Imran and Kumar, Munish and Hussain, Sumaira and Ullah, Inam and Zhang, Xingming},
	year = {2022},
	keywords = {Biometric, Classification, Convolutional neural network, Deep learning, Finger vein recognition, Transfer learning, Xception},
	pages = {116288},
	file = {Shaheed et al. - 2022 - DS-CNN A pre-trained Xception model based on dept.pdf:/Users/vainogranat/Zotero/storage/JZQFKUNT/Shaheed et al. - 2022 - DS-CNN A pre-trained Xception model based on dept.pdf:application/pdf},
}

@misc{zhang_hello_2017,
	title = {Hello {Edge}: {Keyword} {Spotting} on {Microcontrollers}},
	shorttitle = {Hello {Edge}},
	url = {https://arxiv.org/abs/1711.07128v3},
	abstract = {Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4\%, which is {\textasciitilde}10\% higher than the DNN model with similar number of parameters.},
	language = {en},
	urldate = {2024-10-08},
	journal = {arXiv.org},
	author = {Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
	month = nov,
	year = {2017},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/KGBZ86Q2/Zhang et al. - 2017 - Hello Edge Keyword Spotting on Microcontrollers.pdf:application/pdf},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	doi = {10.48550/arXiv.1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv:1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/CUB8Y3PB/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/7WPBZVHJ/1405.html:text/html},
}

@misc{koizumi_description_2020,
	title = {Description and {Discussion} on {DCASE2020} {Challenge} {Task2}: {Unsupervised} {Anomalous} {Sound} {Detection} for {Machine} {Condition} {Monitoring}},
	shorttitle = {Description and {Discussion} on {DCASE2020} {Challenge} {Task2}},
	url = {http://arxiv.org/abs/2006.05822},
	doi = {10.48550/arXiv.2006.05822},
	abstract = {In this paper, we present the task description and discuss the results of the DCASE 2020 Challenge Task 2: Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring. The goal of anomalous sound detection (ASD) is to identify whether the sound emitted from a target machine is normal or anomalous. The main challenge of this task is to detect unknown anomalous sounds under the condition that only normal sound samples have been provided as training data. We have designed this challenge as the first benchmark of ASD research, which includes a large-scale dataset, evaluation metrics, and a simple baseline system. We received 117 submissions from 40 teams, and several novel approaches have been developed as a result of this challenge. On the basis of the analysis of the evaluation results, we discuss two new approaches and their problems.},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Koizumi, Yuma and Kawaguchi, Yohei and Imoto, Keisuke and Nakamura, Toshiki and Nikaido, Yuki and Tanabe, Ryo and Purohit, Harsh and Suefusa, Kaori and Endo, Takashi and Yasuda, Masahiro and Harada, Noboru},
	month = aug,
	year = {2020},
	note = {arXiv:2006.05822},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/BV9K47E7/Koizumi et al. - 2020 - Description and Discussion on DCASE2020 Challenge .pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/2TTS7ZUU/2006.html:text/html},
}

@misc{kao_demystifying_2022,
	title = {Demystifying {Map} {Space} {Exploration} for {NPUs}},
	url = {http://arxiv.org/abs/2210.03731},
	doi = {10.48550/arXiv.2210.03731},
	abstract = {Map Space Exploration is the problem of finding optimized mappings of a Deep Neural Network (DNN) model on an accelerator. It is known to be extremely computationally expensive, and there has been active research looking at both heuristics and learning-based methods to make the problem computationally tractable. However, while there are dozens of mappers out there (all empirically claiming to find better mappings than others), the research community lacks systematic insights on how different search techniques navigate the map-space and how different mapping axes contribute to the accelerator's performance and efficiency. Such insights are crucial to developing mapping frameworks for emerging DNNs that are increasingly irregular (due to neural architecture search) and sparse, making the corresponding map spaces much more complex. In this work, rather than proposing yet another mapper, we do a first-of-its-kind apples-to-apples comparison of search techniques leveraged by different mappers. Next, we extract the learnings from our study and propose two new techniques that can augment existing mappers -- warm-start and sparsity-aware -- that demonstrate speedups, scalability, and robustness across diverse DNN models.},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Kao, Sheng-Chun and Parashar, Angshuman and Tsai, Po-An and Krishna, Tushar},
	month = oct,
	year = {2022},
	note = {arXiv:2210.03731},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/M7MKQRHN/Kao et al. - 2022 - Demystifying Map Space Exploration for NPUs.pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/66EQ2T2N/2210.html:text/html},
}

@misc{akintoye_hybrid_2022,
	title = {A {Hybrid} {Parallelization} {Approach} for {Distributed} and {Scalable} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.05035},
	doi = {10.48550/arXiv.2104.05035},
	abstract = {Recently, Deep Neural Networks (DNNs) have recorded great success in handling medical and other complex classification tasks. However, as the sizes of a DNN model and the available dataset increase, the training process becomes more complex and computationally intensive, which usually takes a longer time to complete. In this work, we have proposed a generic full end-to-end hybrid parallelization approach combining both model and data parallelism for efficiently distributed and scalable training of DNN models. We have also proposed a Genetic Algorithm based heuristic resources allocation mechanism (GABRA) for optimal distribution of partitions on the available GPUs for computing performance optimization. We have applied our proposed approach to a real use case based on 3D Residual Attention Deep Neural Network (3D-ResAttNet) for efficient Alzheimer Disease (AD) diagnosis on multiple GPUs. The experimental evaluation shows that the proposed approach is efficient and scalable, which achieves almost linear speedup with little or no differences in accuracy performance when compared with the existing non-parallel DNN models.},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Akintoye, Samson B. and Han, Liangxiu and Zhang, Xin and Chen, Haoming and Zhang, Daoqiang},
	month = feb,
	year = {2022},
	note = {arXiv:2104.05035},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/UKDYH2A7/Akintoye et al. - 2022 - A Hybrid Parallelization Approach for Distributed .pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/WI3AJ5FW/2104.html:text/html},
}

@misc{jia_beyond_2018,
	title = {Beyond {Data} and {Model} {Parallelism} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1807.05358},
	doi = {10.48550/arXiv.1807.05358},
	abstract = {The computational requirements for training deep neural networks (DNNs) have grown to the point that it is now standard practice to parallelize training. Existing deep learning systems commonly use data or model parallelism, but unfortunately, these strategies often result in suboptimal parallelization performance. In this paper, we define a more comprehensive search space of parallelization strategies for DNNs called SOAP, which includes strategies to parallelize a DNN in the Sample, Operation, Attribute, and Parameter dimensions. We also propose FlexFlow, a deep learning framework that uses guided randomized search of the SOAP space to find a fast parallelization strategy for a specific parallel machine. To accelerate this search, FlexFlow introduces a novel execution simulator that can accurately predict a parallelization strategy's performance and is three orders of magnitude faster than prior approaches that have to execute each strategy. We evaluate FlexFlow with six real-world DNN benchmarks on two GPU clusters and show that FlexFlow can increase training throughput by up to 3.8x over state-of-the-art approaches, even when including its search time, and also improves scalability.},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
	month = jul,
	year = {2018},
	note = {arXiv:1807.05358},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/XL4EYQN4/Jia et al. - 2018 - Beyond Data and Model Parallelism for Deep Neural .pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/2W233IJU/1807.html:text/html},
}

@article{chen_eyeriss_2017,
	title = {Eyeriss: {An} {Energy}-{Efficient} {Reconfigurable} {Accelerator} for {Deep} {Convolutional} {Neural} {Networks}},
	volume = {52},
	issn = {1558-173X},
	shorttitle = {Eyeriss},
	url = {https://ieeexplore.ieee.org/document/7738524},
	doi = {10.1109/JSSC.2016.2616357},
	abstract = {Eyeriss is an accelerator for state-of-the-art deep convolutional neural networks (CNNs). It optimizes for the energy efficiency of the entire system, including the accelerator chip and off-chip DRAM, for various CNN shapes by reconfiguring the architecture. CNNs are widely used in modern AI systems but also bring challenges on throughput and energy efficiency to the underlying hardware. This is because its computation requires a large amount of data, creating significant data movement from on-chip and off-chip that is more energy-consuming than computation. Minimizing data movement energy cost for any CNN shape, therefore, is the key to high throughput and energy efficiency. Eyeriss achieves these goals by using a proposed processing dataflow, called row stationary (RS), on a spatial architecture with 168 processing elements. RS dataflow reconfigures the computation mapping of a given shape, which optimizes energy efficiency by maximally reusing data locally to reduce expensive data movement, such as DRAM accesses. Compression and data gating are also applied to further improve energy efficiency. Eyeriss processes the convolutional layers at 35 frames/s and 0.0029 DRAM access/multiply and accumulation (MAC) for AlexNet at 278 mW (batch size N = 4), and 0.7 frames/s and 0.0035 DRAM access/MAC for VGG-16 at 236 mW (N = 3).},
	number = {1},
	urldate = {2024-10-21},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S. and Sze, Vivienne},
	month = jan,
	year = {2017},
	note = {Conference Name: IEEE Journal of Solid-State Circuits},
	keywords = {Clocks, Computer architecture, Convolutional neural networks (CNNs), dataflow processing, deep learning, energy-efficient accelerators, Hardware, Neural networks, Random access memory, Shape, spatial architecture, Throughput},
	pages = {127--138},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/HY3AT2VR/Chen et al. - 2017 - Eyeriss An Energy-Efficient Reconfigurable Accele.pdf:application/pdf},
}

@misc{noauthor_neurocube_nodate,
	title = {Neurocube: a programmable digital neuromorphic architecture with high-density {3D} memory: {ACM} {SIGARCH} {Computer} {Architecture} {News}: {Vol} 44, {No} 3},
	url = {https://dl.acm.org/doi/abs/10.1145/3007787.3001178},
	urldate = {2024-10-21},
	file = {Neurocube\: a programmable digital neuromorphic architecture with high-density 3D memory\: ACM SIGARCH Computer Architecture News\: Vol 44, No 3:/Users/vainogranat/Zotero/storage/PP8XVUGZ/3007787.html:text/html},
}

@inproceedings{lu_flexflow_2017,
	title = {{FlexFlow}: {A} {Flexible} {Dataflow} {Accelerator} {Architecture} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{FlexFlow}},
	url = {https://ieeexplore.ieee.org/document/7920855},
	doi = {10.1109/HPCA.2017.29},
	abstract = {Convolutional Neural Networks (CNN) are very computation-intensive. Recently, a lot of CNN accelerators based on the CNN intrinsic parallelism are proposed. However, we observed that there is a big mismatch between the parallel types supported by computing engine and the dominant parallel types of CNN workloads. This mismatch seriously degrades resource utilization of existing accelerators. In this paper, we propose a flexible dataflow architecture (FlexFlow) that can leverage the complementary effects among feature map, neuron, and synapse parallelism to mitigate the mismatch. We evaluated our design with six typical practical workloads, it acquires 2-10x performance speedup and 2.5-10x power efficiency improvement compared with three state-of-the-art accelerator architectures. Meanwhile, FlexFlow is highly scalable with growing computing engine scale.},
	urldate = {2024-10-21},
	booktitle = {2017 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Lu, Wenyan and Yan, Guihai and Li, Jiajun and Gong, Shijun and Han, Yinhe and Li, Xiaowei},
	month = feb,
	year = {2017},
	note = {ISSN: 2378-203X},
	keywords = {Clocks, Computer architecture, Accelerator, Biological neural networks, Complementary Effect, Convolutional Neural Networks, Flexible Dataflow, Kernel, Neurons, Parallel processing, Pipelines},
	pages = {553--564},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/AH8L8XBG/Lu et al. - 2017 - FlexFlow A Flexible Dataflow Accelerator Architec.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/vainogranat/Zotero/storage/PG8AUH87/7920855.html:text/html},
}

@misc{seshadri_evaluation_2022,
	title = {An {Evaluation} of {Edge} {TPU} {Accelerators} for {Convolutional} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2102.10423},
	doi = {10.48550/arXiv.2102.10423},
	abstract = {Edge TPUs are a domain of accelerators for low-power, edge devices and are widely used in various Google products such as Coral and Pixel devices. In this paper, we first discuss the major microarchitectural details of Edge TPUs. Then, we extensively evaluate three classes of Edge TPUs, covering different computing ecosystems, that are either currently deployed in Google products or are the product pipeline, across 423K unique convolutional neural networks. Building upon this extensive study, we discuss critical and interpretable microarchitectural insights about the studied classes of Edge TPUs. Mainly, we discuss how Edge TPU accelerators perform across convolutional neural networks with different structures. Finally, we present our ongoing efforts in developing high-accuracy learned machine learning models to estimate the major performance metrics of accelerators such as latency and energy consumption. These learned models enable significantly faster (in the order of milliseconds) evaluations of accelerators as an alternative to time-consuming cycle-accurate simulators and establish an exciting opportunity for rapid hard-ware/software co-design.},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Seshadri, Kiran and Akin, Berkin and Laudon, James and Narayanaswami, Ravi and Yazdanbakhsh, Amir},
	month = oct,
	year = {2022},
	note = {arXiv:2102.10423},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/KQFV8733/Seshadri et al. - 2022 - An Evaluation of Edge TPU Accelerators for Convolu.pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/SIJCKTSX/2102.html:text/html},
}

@misc{guo_analyzing_2023,
	title = {Analyzing {Quantization} in {TVM}},
	url = {http://arxiv.org/abs/2308.10905},
	doi = {10.48550/arXiv.2308.10905},
	abstract = {There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50\% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of tasks: computation-bound and memory-bound, and provide a detailed comparison of various optimization techniques in TVM. Through the identification of performance issues, we have successfully improved quantization by addressing a bug in graph building. Furthermore, we analyze multiple optimization strategies to achieve the optimal quantization result. The best experiment achieves 163.88\% improvement compared with the TVM compiled baseline in inference time for the compute-bound task and 194.98\% for the memory-bound task.},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Guo, Mingfei},
	month = aug,
	year = {2023},
	note = {arXiv:2308.10905},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, TVM},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/74L5N3JA/Guo - 2023 - Analyzing Quantization in TVM.pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/KRV5W39R/2308.html:text/html},
}

@misc{krishnamoorthi_quantizing_2018,
	title = {Quantizing deep convolutional networks for efficient inference: {A} whitepaper},
	shorttitle = {Quantizing deep convolutional networks for efficient inference},
	url = {http://arxiv.org/abs/1806.08342},
	doi = {10.48550/arXiv.1806.08342},
	abstract = {We present an overview of techniques for quantizing convolutional neural networks for inference with integer weights and activations. Per-channel quantization of weights and per-layer quantization of activations to 8-bits of precision post-training produces classification accuracies within 2\% of floating point networks for a wide variety of CNN architectures. Model sizes can be reduced by a factor of 4 by quantizing weights to 8-bits, even when 8-bit arithmetic is not supported. This can be achieved with simple, post training quantization of weights.We benchmark latencies of quantized networks on CPUs and DSPs and observe a speedup of 2x-3x for quantized implementations compared to floating point on CPUs. Speedups of up to 10x are observed on specialized processors with fixed point SIMD capabilities, like the Qualcomm QDSPs with HVX. Quantization-aware training can provide further improvements, reducing the gap to floating point to 1\% at 8-bit precision. Quantization-aware training also allows for reducing the precision of weights to four bits with accuracy losses ranging from 2\% to 10\%, with higher accuracy drop for smaller networks.We introduce tools in TensorFlow and TensorFlowLite for quantizing convolutional networks and review best practices for quantization-aware training to obtain high accuracy with quantized weights and activations. We recommend that per-channel quantization of weights and per-layer quantization of activations be the preferred quantization scheme for hardware acceleration and kernel optimization. We also propose that future processors and hardware accelerators for optimized inference support precisions of 4, 8 and 16 bits.},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Krishnamoorthi, Raghuraman},
	month = jun,
	year = {2018},
	note = {arXiv:1806.08342},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/RAXNH8L6/Krishnamoorthi - 2018 - Quantizing deep convolutional networks for efficie.pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/YS4VWNEX/1806.html:text/html},
}

@misc{jacob_quantization_2017,
	title = {Quantization and {Training} of {Neural} {Networks} for {Efficient} {Integer}-{Arithmetic}-{Only} {Inference}},
	url = {http://arxiv.org/abs/1712.05877},
	doi = {10.48550/arXiv.1712.05877},
	abstract = {The rising popularity of intelligent mobile devices and the daunting computational cost of deep learning-based models call for efficient and accurate on-device inference schemes. We propose a quantization scheme that allows inference to be carried out using integer-only arithmetic, which can be implemented more efficiently than floating point inference on commonly available integer-only hardware. We also co-design a training procedure to preserve end-to-end model accuracy post quantization. As a result, the proposed quantization scheme improves the tradeoff between accuracy and on-device latency. The improvements are significant even on MobileNets, a model family known for run-time efficiency, and are demonstrated in ImageNet classification and COCO detection on popular CPUs.},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
	month = dec,
	year = {2017},
	note = {arXiv:1712.05877},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/UZY6BX24/Jacob et al. - 2017 - Quantization and Training of Neural Networks for E.pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/HZU7SWJW/1712.html:text/html},
}

@misc{silvano_survey_2024,
	title = {A {Survey} on {Deep} {Learning} {Hardware} {Accelerators} for {Heterogeneous} {HPC} {Platforms}},
	url = {http://arxiv.org/abs/2306.15552},
	doi = {10.48550/arXiv.2306.15552},
	abstract = {Recent trends in deep learning (DL) imposed hardware accelerators as the most viable solution for several classes of high-performance computing (HPC) applications such as image classification, computer vision, and speech recognition. This survey summarizes and classifies the most recent advances in designing DL accelerators suitable to reach the performance requirements of HPC applications. In particular, it highlights the most advanced approaches to support deep learning accelerations including not only GPU and TPU-based accelerators but also design-specific hardware accelerators such as FPGA-based and ASIC-based accelerators, Neural Processing Units, open hardware RISC-V-based accelerators and co-processors. The survey also describes accelerators based on emerging memory technologies and computing paradigms, such as 3D-stacked Processor-In-Memory, non-volatile memories (mainly, Resistive RAM and Phase Change Memories) to implement in-memory computing, Neuromorphic Processing Units, and accelerators based on Multi-Chip Modules. Among emerging technologies, we also include some insights into quantum-based accelerators and photonics. To conclude, the survey classifies the most influential architectures and technologies proposed in the last years, with the purpose of offering the reader a comprehensive perspective in the rapidly evolving field of deep learning.},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Silvano, Cristina and Ielmini, Daniele and Ferrandi, Fabrizio and Fiorin, Leandro and Curzel, Serena and Benini, Luca and Conti, Francesco and Garofalo, Angelo and Zambelli, Cristian and Calore, Enrico and Schifano, Sebastiano Fabio and Palesi, Maurizio and Ascia, Giuseppe and Patti, Davide and Petra, Nicola and Caro, Davide De and Lavagno, Luciano and Urso, Teodoro and Cardellini, Valeria and Cardarilli, Gian Carlo and Birke, Robert and Perri, Stefania},
	month = jul,
	year = {2024},
	note = {arXiv:2306.15552},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Emerging Technologies},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/MUPJUC3N/Silvano et al. - 2024 - A Survey on Deep Learning Hardware Accelerators fo.pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/SKE89XEK/2306.html:text/html},
}

@inproceedings{watkins_transitioning_2007,
	title = {Transitioning from federated avionics architectures to {Integrated} {Modular} {Avionics}},
	url = {https://ieeexplore.ieee.org/document/4391842},
	doi = {10.1109/DASC.2007.4391842},
	abstract = {This paper identifies considerations for transitioning from a federated avionics architecture to an integrated modular avionics (IMA) architecture. Federated avionics architectures make use of distributed avionics functions that are packaged as self-contained units (LRUs and LRMs). IMA architectures employ a high-integrity, partitioned environment that hosts multiple avionics functions of different criticalities on a shared computing platform. This provides for weight and power savings since computing resources can be used more efficiently. This paper establishes the benefits of transitioning to IMA. To aid in the planning process, the paper also identifies factors to consider before transitioning to IMA. The approach to resource management (computing, communication, and I/O) is identified as the fundamental architectural difference between federated and IMA systems. The paper describes how this difference changes the development process and benefits the systems integrator. This paper also addresses misconceptions about the resource management mechanisms that can occur during a transition to IMA and concludes that resources are not inherently constrained by IMA architectures. Guidance is provided for transitioning to both "open" and "closed" IMA architectures. Open IMA architectures utilize open interface standards that are available in the public domain. Closed IMA architectures utilize proprietary interfaces that can be customized. The analysis of these avionics architectures is based upon the authors' experience in developing platform computing systems at GE Aviation. GE Aviation has developed open system IMA architectures for commercial aircraft (Boeing 787 Dreamliner), as well as military aircraft (Boeing C-130 combat aircraft, and Boeing KC-767 Tanker).},
	urldate = {2024-10-28},
	booktitle = {2007 {IEEE}/{AIAA} 26th {Digital} {Avionics} {Systems} {Conference}},
	author = {Watkins, Christopher B. and Walter, Randy},
	month = oct,
	year = {2007},
	note = {ISSN: 2155-7209},
	keywords = {Computer architecture, Aerospace electronics, Communication channels, Communication system control, Military aircraft, Packaging, Power system planning, Process planning, Resource management, User interfaces, SOCs},
	pages = {2.A.1--1--2.A.1--10},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/LMGVLZ6P/Watkins and Walter - 2007 - Transitioning from federated avionics architecture.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/vainogranat/Zotero/storage/D7MRFDNE/4391842.html:text/html},
}

@article{di_natale_moving_2010,
	title = {Moving {From} {Federated} to {Integrated} {Architectures} in {Automotive}: {The} {Role} of {Standards}, {Methods} and {Tools}},
	volume = {98},
	issn = {1558-2256},
	shorttitle = {Moving {From} {Federated} to {Integrated} {Architectures} in {Automotive}},
	url = {https://ieeexplore.ieee.org/document/5440059},
	doi = {10.1109/JPROC.2009.2039550},
	abstract = {Cost pressure, flexibility, extensibility and the need for coping with increased functional complexity are changing the fundamental paradigms for the definition of automotive and aeronautics architectures. Traditional designs are based on the concept of a Federated Architecture in which integrated hardware/software components [Electronic Control Units (ECUs)] realize mostly independent or loosely interconnected functions. These components are connected by bus and cooperate by exchanging messages. This paradigm is now being replaced by the Integrated Architecture,—the concept comes from Integrated Modular Avionics (IMA) introduced by the avionics community (see C. B. Watkins and R. Walter, “Transitioning from federated avionics architectures to integrated modular avionics,” in Proc. 26th Digital Avionics Syst. Conf., Oct. 2007) but it is certainly general and applicable to other fields and in particular, automotive—in which software components can be supplied from multiple sources, integrated on the same hardware platform or physically distributed and possibly moved from one CPU to another without loss of functional and time correctness and providing a guaranteed level of reliability. This shift will decouple software design from the hardware platform design and provide opportunities for the optimization of the architecture configuration, increased extensibility, flexibility and modularity. However, the integration of software components in a distributed system realizing a complex functional behavior and characterized by safety, time and reliability constraints requires a much tighter control on the component model and its semantics, new methods and tools for analyzing the results of the composition, whether by simulation or formal methods, and methods for exploring the architecture solution space and optimizing the configuration. We provide a general overview of existing challenges and possible solutions to the design and analysis problem, with special focus on the automotive domain. The development of such methods and tools must necessarily consider compatibility with existing modeling languages and standards, including UML, AUTOSAR and synchronous reactive models, on which the widely used commercial products Simulink and SCADE are based.},
	number = {4},
	urldate = {2024-10-28},
	journal = {Proceedings of the IEEE},
	author = {Di Natale, Marco and Sangiovanni-Vincentelli, Alberto Luigi},
	month = apr,
	year = {2010},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Computer architecture, Hardware, Aerospace electronics, Architecture, automotive electronic system, Automotive engineering, automotive software, Cost function, design methodology, Design optimization, design space exploration, Software design, Software safety, Software tools, system design, Time factors, SOCs},
	pages = {603--620},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/I7KRS72J/Di Natale and Sangiovanni-Vincentelli - 2010 - Moving From Federated to Integrated Architectures .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/vainogranat/Zotero/storage/YX55HQIZ/5440059.html:text/html},
}

@article{bandur_making_2021,
	title = {Making the {Case} for {Centralized} {Automotive} {E}/{E} {Architectures}},
	volume = {70},
	issn = {1939-9359},
	url = {https://ieeexplore.ieee.org/document/9337216},
	doi = {10.1109/TVT.2021.3054934},
	abstract = {The rapidly increasing complexity of software in modern cars dictates new trends in electrical and/or electronic (E/E) automotive architectures. As a result, many original equipment manufacturers (OEMs) and suppliers have been advocating centralized E/E architectures as the automotive architectures of the future. In this article we make the case for centralized E/E architectures in the automotive industry. We discuss the motivation for centralized architectural schemes by carefully examining challenges and drawbacks of the traditional decentralized automotive E/E architectures, while contrasting with the corresponding benefits offered by centralization. Then, the technologies required to support new centralized architectures are discussed in detail. In particular, we present the state of the art in networking technologies, virtualization, electronic control unit (ECU) hardware and AUTOSAR, and discuss the state of adoption of these technologies in industry. Throughout, functional safety is considered and addressed as an overarching concern in the automotive industry.},
	number = {2},
	urldate = {2024-10-28},
	journal = {IEEE Transactions on Vehicular Technology},
	author = {Bandur, Victor and Selim, Gehan and Pantelic, Vera and Lawford, Mark},
	month = feb,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Vehicular Technology},
	keywords = {Computer architecture, Hardware, Architecture, Automotive engineering, Automotive E/E architecture, centralization, domain control, functional safety, ISO 26 262, Market research, Safety, Software, SOCs},
	pages = {1230--1245},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/SZC4S94B/Bandur et al. - 2021 - Making the Case for Centralized Automotive EE Arc.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/vainogranat/Zotero/storage/B27L2TBT/9337216.html:text/html},
}

@article{banerjee_3-d_2001,
	title = {3-{D} {ICs}: {A} novel chip design for improving deep-submieroraeter interconnect performance and systems-on-chip integration and systems-on-chlp integration},
	volume = {89},
	shorttitle = {3-{D} {ICs}},
	doi = {10.1109/5.929647},
	abstract = {Performance of deep-submicrometer very large scale integrated (VLSI) circuits is being increasingly dominated by the interconnects due to decreasing wire pilch and increasing die size. Additionally, heterogeneous integration of different technologies in one single chip is becoming increasingly desirable, for which planar (two-dimensional) Its may not be suitable. This paper analyzes the limitations of the existing interconnect technologies and design methodologies and presents a novel three-dimensional (3-D) chip design strategy that exploits the vertical dimension to alleviate the interconnect related problems and to facilitate heterogeneous integration of technologies to realize a system-on-a-chip (SoC) design. A comprehensive analytical treatment of these 3-D ICs has been presented and it has been shown that by simply dividing a planar chip into separate blocks, each occupying a separatephvsical level interconnected by short and vertical interlay er interconnects (VILICs), significant improvement in performance and reduction in wire-limited chip area can be achieved, without the aid of any other circuit or design innovations. A scheme to optimize the interconnect distribution among different interconnect tiers is presented and the effect of transferring the repeaters to upper Si layers has been quantified in this analysis for a two-layer 3-D chip. Furthermore, one of the major concerns in 3-D ICs arising due to power dissipation problems has been analyzed and an analytical model has been presented to estimate the temperatures of the different active layers. It is demonstrated that advancement in heat sinking technology will be necessary in order to extract maximum performance from these chips. Implications of 3-D device architecture on several design issues have also been discussed with especial attention to SoC design strategies. Finally, some of the promising technologies for manufacturing 3-D ICs have been outlined. © 2001 IEEE.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Banerjee, K. and Souri, S.J. and Kapur, P. and Saraswat, K.C.},
	year = {2001},
	keywords = {3-d ICs, Heterogeneous integration, Interconnect performance, Optical i/os, Power dissipation, System interconnects, System-on-a-chip design, VLSI design},
	pages = {602--632},
	annote = {Cited By :868},
}

@misc{noauthor_3-d_nodate,
	title = {3-{D} {ICs}: a novel chip design for improving deep-submicrometer interconnect performance and systems-on-chip integration {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore} - ieeexplore.ieee.org},
	url = {https://ieeexplore.ieee.org/document/929647},
	urldate = {2024-10-28},
}

@inproceedings{esmaeilzadeh_dark_2011,
	title = {Dark silicon and the end of multicore scaling},
	url = {https://ieeexplore.ieee.org/document/6307773},
	abstract = {Since 2005, processor designers have increased core counts to exploit Moore's Law scaling, rather than focusing on single-core performance. The failure of Dennard scaling, to which the shift to multicore parts is partially a response, may soon limit multicore scaling just as single-core scaling has been curtailed. This paper models multicore scaling limits by combining device scaling, single-core scaling, and multicore scaling to measure the speedup potential for a set of parallel workloads for the next five technology generations. For device scaling, we use both the ITRS projections and a set of more conservative device scaling parameters. To model single-core scaling, we combine measurements from over 150 processors to derive Pareto-optimal frontiers for area/performance and power/performance. Finally, to model multicore scaling, we build a detailed performance model of upper-bound performance and lower-bound core power. The multicore designs we study include single-threaded CPU-like and massively threaded GPU-like multicore chip organizations with symmetric, asymmetric, dynamic, and composed topologies. The study shows that regardless of chip organization and topology, multicore scaling is power limited to a degree not widely appreciated by the computing community. Even at 22 nm (just one year from now), 21\% of a fixed-size chip must be powered off, and at 8 nm, this number grows to more than 50\%. Through 2024, only 7.9× average speedup is possible across commonly used parallel workloads, leaving a nearly 24-fold gap from a target of doubled performance per generation.},
	urldate = {2024-10-28},
	booktitle = {2011 38th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Esmaeilzadeh, Hadi and Blem, Emily and Amant, Renée St. and Sankaralingam, Karthikeyan and Burger, Doug},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6897},
	keywords = {Dark Silicon, Instruction sets, Microarchitecture, Modeling, Multicore, Multicore processing, Organizations, Performance evaluation, Power, Technology Scaling, Topology, Transistors},
	pages = {365--376},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/KBCMQJAI/Esmaeilzadeh et al. - 2011 - Dark silicon and the end of multicore scaling.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/vainogranat/Zotero/storage/354BJDTJ/6307773.html:text/html},
}

@article{hill_amdahls_2008,
	title = {Amdahl's {Law} in the {Multicore} {Era}},
	volume = {41},
	issn = {1558-0814},
	url = {https://ieeexplore.ieee.org/document/4563876},
	doi = {10.1109/MC.2008.209},
	abstract = {Augmenting Amdahl's law with a corollary for multicore hardware makes it relevant to future generations of chips with multiple processor cores. Obtaining optimal multicore performance will require further research in both extracting more parallelism and making sequential cores faster.},
	number = {7},
	urldate = {2024-10-28},
	journal = {Computer},
	author = {Hill, Mark D. and Marty, Michael R.},
	month = jul,
	year = {2008},
	note = {Conference Name: Computer},
	keywords = {Computer architecture, Hardware, Parallel processing, Pipelines, SOCs, Multicore processing, Amdahl's law, chip multiprocessors (CMPs), Costs, Energy management, Equations, multicore chips, Multiprocessor interconnection networks, Roads},
	pages = {33--38},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/EYYFSQLL/Hill and Marty - 2008 - Amdahl's Law in the Multicore Era.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/vainogranat/Zotero/storage/A9A76U59/4563876.html:text/html},
}

@inproceedings{chung_single-chip_2010,
	title = {Single-{Chip} {Heterogeneous} {Computing}: {Does} the {Future} {Include} {Custom} {Logic}, {FPGAs}, and {GPGPUs}?},
	shorttitle = {Single-{Chip} {Heterogeneous} {Computing}},
	url = {https://ieeexplore.ieee.org/abstract/document/5695539},
	doi = {10.1109/MICRO.2010.36},
	abstract = {To extend the exponential performance scaling of future chip multiprocessors, improving energy efficiency has become a first-class priority. Single-chip heterogeneous computing has the potential to achieve greater energy efficiency by combining traditional processors with unconventional cores (U-cores) such as custom logic, FPGAs, or GPGPUs. Although U-cores are effective at increasing performance, their benefits can also diminish given the scarcity of projected bandwidth in the future. To understand the relative merits between different approaches in the face of technology constraints, this work builds on prior modeling of heterogeneous multicores to support U-cores. Unlike prior models that trade performance, power, and area using well-known relationships between simple and complex processors, our model must consider the less-obvious relationships between conventional processors and a diverse set of U-cores. Further, our model supports speculation of future designs from scaling trends predicted by the ITRS road map. The predictive power of our model depends upon U-core-specific parameters derived by measuring performance and power of tuned applications on today's state-of-the-art multicores, GPUs, FPGAs, and ASICs. Our results reinforce some current-day understandings of the potential and limitations of U-cores and also provides new insights on their relative merits.},
	urldate = {2024-10-28},
	booktitle = {2010 43rd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	author = {Chung, Eric S. and Milder, Peter A. and Hoe, James C. and Mai, Ken},
	month = dec,
	year = {2010},
	note = {ISSN: 2379-3155},
	keywords = {Multicore processing, Application specific integrated circuits, asic, Bandwidth, Computational modeling, Field programmable gate arrays, fpga, gpu, Graphics processing unit, heterogeneous, itrs, multicore, technology scaling},
	pages = {225--236},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/9XWZLZEX/Chung et al. - 2010 - Single-Chip Heterogeneous Computing Does the Futu.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/vainogranat/Zotero/storage/MZV5S6WG/5695539.html:text/html},
}

@article{benini_networks_2002,
	title = {Networks on chips: a new {SoC} paradigm},
	volume = {35},
	issn = {1558-0814},
	shorttitle = {Networks on chips},
	url = {https://ieeexplore.ieee.org/document/976921},
	doi = {10.1109/2.976921},
	abstract = {On-chip micronetworks, designed with a layered methodology, will meet the distinctive challenges of providing functionally correct, reliable operation of interacting system-on-chip components. A system on chip (SoC) can provide an integrated solution to challenging design problems in the telecommunications, multimedia, and consumer electronics domains. Much of the progress in these fields hinges on the designers' ability to conceive complex electronic engines under strong time-to-market pressure. Success will require using appropriate design and process technologies, as well as interconnecting existing components reliably in a plug-and-play fashion. Focusing on using probabilistic metrics such as average values or variance to quantify design objectives such as performance and power will lead to a major change in SoC design methodologies. Overall, these designs will be based on both deterministic and stochastic models. Creating complex SoCs requires a modular, component-based approach to both hardware and software design. Despite numerous challenges, the authors believe that developers will solve the problems of designing SoC networks. At the same time, they believe that a layered micronetwork design methodology will likely be the only path to mastering the complexity of future SoC designs.},
	number = {1},
	urldate = {2024-10-28},
	journal = {Computer},
	author = {Benini, L. and De Micheli, G.},
	month = jan,
	year = {2002},
	note = {Conference Name: Computer},
	keywords = {Consumer electronics, Design methodology, Engines, Fasteners, Multimedia systems, Network-on-a-chip, Process design, System-on-a-chip, Telecommunication network reliability, Time to market},
	pages = {70--78},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/5HXURKGY/Benini and De Micheli - 2002 - Networks on chips a new SoC paradigm.pdf:application/pdf},
}

@article{sze_efficient_2017,
	title = {Efficient {Processing} of {Deep} {Neural} {Networks}: {A} {Tutorial} and {Survey}},
	volume = {105},
	issn = {1558-2256},
	shorttitle = {Efficient {Processing} of {Deep} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8114708},
	doi = {10.1109/JPROC.2017.2761740},
	abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
	number = {12},
	urldate = {2024-10-28},
	journal = {Proceedings of the IEEE},
	author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
	month = dec,
	year = {2017},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {Computer architecture, dataflow processing, deep learning, energy-efficient accelerators, Neural networks, Biological neural networks, Neurons, Artificial intelligence, ASIC, Benchmark testing, computer architecture, convolutional neural networks, Convolutional neural networks, deep neural networks, low power, machine learning, Machine learning, spatial architectures, Tutorials, VLSI},
	pages = {2295--2329},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/DEGBWVKK/Sze et al. - 2017 - Efficient Processing of Deep Neural Networks A Tu.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/vainogranat/Zotero/storage/FZD7EP6K/8114708.html:text/html},
}

@article{chen_survey_2020,
	title = {A {Survey} of {Accelerator} {Architectures} for {Deep} {Neural} {Networks}},
	volume = {6},
	issn = {2095-8099},
	url = {https://www.sciencedirect.com/science/article/pii/S2095809919306356},
	doi = {10.1016/j.eng.2020.01.007},
	abstract = {Recently, due to the availability of big data and the rapid growth of computing power, artificial intelligence (AI) has regained tremendous attention and investment. Machine learning (ML) approaches have been successfully applied to solve many problems in academia and in industry. Although the explosion of big data applications is driving the development of ML, it also imposes severe challenges of data processing speed and scalability on conventional computer systems. Computing platforms that are dedicatedly designed for AI applications have been considered, ranging from a complement to von Neumann platforms to a “must-have” and stand-alone technical solution. These platforms, which belong to a larger category named “domain-specific computing,” focus on specific customization for AI. In this article, we focus on summarizing the recent advances in accelerator designs for deep neural networks (DNNs)—that is, DNN accelerators. We discuss various architectures that support DNN executions in terms of computing units, dataflow optimization, targeted network topologies, architectures on emerging technologies, and accelerators for emerging applications. We also provide our visions on the future trend of AI chip designs.},
	number = {3},
	urldate = {2024-10-29},
	journal = {Engineering},
	author = {Chen, Yiran and Xie, Yuan and Song, Linghao and Chen, Fan and Tang, Tianqi},
	month = mar,
	year = {2020},
	keywords = {Accelerator, Deep neural network, Domain-specific architecture},
	pages = {264--274},
	file = {ScienceDirect Snapshot:/Users/vainogranat/Zotero/storage/3PPML6NA/S2095809919306356.html:text/html},
}

@inproceedings{chen_diannao_2014,
	address = {New York, NY, USA},
	series = {{ASPLOS} '14},
	title = {{DianNao}: a small-footprint high-throughput accelerator for ubiquitous machine-learning},
	isbn = {978-1-4503-2305-5},
	shorttitle = {{DianNao}},
	url = {https://doi.org/10.1145/2541940.2541967},
	doi = {10.1145/2541940.2541967},
	abstract = {Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope.Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy.We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.},
	urldate = {2024-10-29},
	booktitle = {Proceedings of the 19th international conference on {Architectural} support for programming languages and operating systems},
	publisher = {Association for Computing Machinery},
	author = {Chen, Tianshi and Du, Zidong and Sun, Ninghui and Wang, Jia and Wu, Chengyong and Chen, Yunji and Temam, Olivier},
	month = feb,
	year = {2014},
	pages = {269--284},
	file = {Chen et al. - 2014 - DianNao a small-footprint high-throughput acceler.pdf:/Users/vainogranat/Zotero/storage/22DACPXV/Chen et al. - 2014 - DianNao a small-footprint high-throughput acceler.pdf:application/pdf},
}

@article{draghici_capabilities_2002,
	title = {On the capabilities of neural networks using limited precision weights},
	volume = {15},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608002000321},
	doi = {10.1016/S0893-6080(02)00032-1},
	abstract = {This paper analyzes some aspects of the computational power of neural networks using integer weights in a very restricted range. Using limited range integer values opens the road for efficient VLSI implementations because: (i) a limited range for the weights can be translated into reduced storage requirements and (ii) integer computation can be implemented in a more efficient way than the floating point one. The paper concentrates on classification problems and shows that, if the weights are restricted in a drastic way (both range and precision), the existence of a solution is not to be taken for granted anymore. The paper presents an existence result which relates the difficulty of the problem as characterized by the minimum distance between patterns of different classes to the weight range necessary to ensure that a solution exists. This result allows us to calculate a weight range for a given category of problems and be confident that the network has the capability to solve the given problems with integer weights in that range. Worst-case lower bounds are given for the number of entropy bits and weights necessary to solve a given problem. Various practical issues such as the relationship between the information entropy bits and storage bits are also discussed. The approach presented here uses a worst-case analysis. Therefore, the approach tends to overestimate the values obtained for the weight range, the number of bits and the number of weights. The paper also presents some statistical considerations that can be used to give up the absolute confidence of a successful training in exchange for values more appropriate for practical use. The approach presented is also discussed in the context of the VC-complexity.},
	number = {3},
	urldate = {2024-10-29},
	journal = {Neural Networks},
	author = {Draghici, Sorin},
	month = apr,
	year = {2002},
	keywords = {Integer weights, VC-complexity, VLSI implementation},
	pages = {395--414},
	file = {Draghici - 2002 - On the capabilities of neural networks using limit.pdf:/Users/vainogranat/Zotero/storage/J4UUDCQ2/Draghici - 2002 - On the capabilities of neural networks using limit.pdf:application/pdf;ScienceDirect Snapshot:/Users/vainogranat/Zotero/storage/8UACPU92/S0893608002000321.html:text/html},
}

@misc{tan_efficientnet_2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	doi = {10.48550/arXiv.1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2024-11-06},
	publisher = {arXiv},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv:1905.11946},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/BVWASVK3/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/IXP96LGI/1905.html:text/html},
}

@incollection{rumelhart_learning_1987,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	isbn = {978-0-262-29140-8},
	url = {https://ieeexplore.ieee.org/document/6302929},
	abstract = {This chapter contains sections titled: The Problem, The Generalized Delta Rule, Simulation Results, Some Further Generalizations, Conclusion},
	urldate = {2024-11-06},
	booktitle = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}: {Foundations}},
	publisher = {MIT Press},
	author = {Rumelhart, David E. and McClelland, James L.},
	year = {1987},
	note = {Conference Name: Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
	pages = {318--362},
	file = {IEEE Xplore Abstract Record:/Users/vainogranat/Zotero/storage/2G4ZX42Y/6302929.html:text/html},
}

@article{chen_auto-encoders_2023,
	title = {Auto-{Encoders} in {Deep} {Learning}—{A} {Review} with {New} {Perspectives}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/11/8/1777},
	doi = {10.3390/math11081777},
	abstract = {Deep learning, which is a subfield of machine learning, has opened a new era for the development of neural networks. The auto-encoder is a key component of deep structure, which can be used to realize transfer learning and plays an important role in both unsupervised learning and non-linear feature extraction. By highlighting the contributions and challenges of recent research papers, this work aims to review state-of-the-art auto-encoder algorithms. Firstly, we introduce the basic auto-encoder as well as its basic concept and structure. Secondly, we present a comprehensive summarization of different variants of the auto-encoder. Thirdly, we analyze and study auto-encoders from three different perspectives. We also discuss the relationships between auto-encoders, shallow models and other deep learning models. The auto-encoder and its variants have successfully been applied in a wide range of fields, such as pattern recognition, computer vision, data generation, recommender systems, etc. Then, we focus on the available toolkits for auto-encoders. Finally, this paper summarizes the future trends and challenges in designing and training auto-encoders. We hope that this survey will provide a good reference when using and designing AE models.},
	language = {en},
	number = {8},
	urldate = {2024-11-06},
	journal = {Mathematics},
	author = {Chen, Shuangshuang and Guo, Wei},
	month = jan,
	year = {2023},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, artificial intelligence, auto-encoder, survey},
	pages = {1777},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/HRMNQ36C/Chen and Guo - 2023 - Auto-Encoders in Deep Learning—A Review with New P.pdf:application/pdf},
}

@misc{palacios_systolic_2024,
	title = {Systolic {Arrays} and {Structured} {Pruning} {Co}-design for {Efficient} {Transformers} in {Edge} {Systems}},
	url = {http://arxiv.org/abs/2411.10285},
	doi = {10.48550/arXiv.2411.10285},
	abstract = {Efficient deployment of resource-intensive transformers on edge devices necessitates cross-stack optimization. We thus study the interrelation between structured pruning and systolic acceleration, matching the size of pruned blocks with the systolic array dimensions. In this setting, computations of pruned weight blocks can be skipped, reducing run-time and energy consumption, but potentially impacting quality of service (QoS). To evaluate the trade-offs between systolic array size and sparsity opportunities, we present a novel co-design framework that integrates algorithmic optimization, system simulation, and hardware design. Targeting speech recognition using transformers as a case study, we analyze how configuration choices across the stack affect performance metrics. Results demonstrate that structured pruning on systems featuring systolic array acceleration can effectively increase performance, while maintaining high QoS levels. Up to 26\% system-wide speedups due to structured pruning were measured, with only 1.4\% word error rate degradation on the standard Librispeech dataset.},
	urldate = {2024-11-19},
	publisher = {arXiv},
	author = {Palacios, Pedro and Medina, Rafael and Rouas, Jean-Luc and Ansaloni, Giovanni and Atienza, David},
	month = nov,
	year = {2024},
	note = {arXiv:2411.10285},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/QPSHZ8BU/Palacios et al. - 2024 - Systolic Arrays and Structured Pruning Co-design f.pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/GFMP6G6S/2411.html:text/html},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2024-11-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:/Users/vainogranat/Zotero/storage/MCXR3SC2/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@misc{wu_integer_2020,
	title = {Integer {Quantization} for {Deep} {Learning} {Inference}: {Principles} and {Empirical} {Evaluation}},
	shorttitle = {Integer {Quantization} for {Deep} {Learning} {Inference}},
	url = {http://arxiv.org/abs/2004.09602},
	doi = {10.48550/arXiv.2004.09602},
	abstract = {Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of quantization parameters and evaluate their choices on a wide range of neural network models for different application domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is able to maintain accuracy within 1\% of the floating-point baseline on all networks studied, including models that are more difficult to quantize, such as MobileNets and BERT-large.},
	urldate = {2024-11-24},
	publisher = {arXiv},
	author = {Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
	month = apr,
	year = {2020},
	note = {arXiv:2004.09602},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/vainogranat/Zotero/storage/W49R7HPM/Wu et al. - 2020 - Integer Quantization for Deep Learning Inference .pdf:application/pdf;Snapshot:/Users/vainogranat/Zotero/storage/VM8TBY9V/2004.html:text/html},
}
